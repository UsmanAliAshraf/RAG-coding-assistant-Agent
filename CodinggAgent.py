# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GAqMNzNkdn94Nt0oTG-AMsyI5cw69ghy

## **1. Load Python Docs PDF and create embeddings and FAISS index**
"""

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from google.colab import files

import os
import json
import pickle
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

# === üîß Config ===
DATA_FOLDER = "Data"
CHUNKS_PATH = "chunks.json"
FAISS_FOLDER = "faiss_index"
USE_EXISTING = os.path.exists(CHUNKS_PATH) and os.path.exists(FAISS_FOLDER)

# === üî¢ Load or Create Chunks ===
if USE_EXISTING:
    print("‚úÖ Using cached chunks and FAISS index...")
    with open(CHUNKS_PATH, "r", encoding="utf-8") as f:
        chunks_raw = json.load(f)
    from langchain.schema import Document
    chunks = [Document(page_content=c["page_content"], metadata=c["metadata"]) for c in chunks_raw]
else:
    print("üìÇ Loading PDFs from folder:", DATA_FOLDER)
    docs = []
    for filename in os.listdir(DATA_FOLDER):
        if filename.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(DATA_FOLDER, filename))
            docs.extend(loader.load())
    print(f"‚úÖ Loaded {len(docs)} documents from {len(os.listdir(DATA_FOLDER))} PDFs")

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = splitter.split_documents(docs)
    print(f"‚úÖ Created {len(chunks)} chunks")

    # Save chunks for reuse
    chunks_raw = [{"page_content": c.page_content, "metadata": c.metadata} for c in chunks]
    with open(CHUNKS_PATH, "w", encoding="utf-8") as f:
        json.dump(chunks_raw, f)

# === üîç Load or Create FAISS ===
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

if USE_EXISTING:
    vectorstore = FAISS.load_local(FAISS_FOLDER, embeddings, allow_dangerous_deserialization=True)
    print("‚úÖ FAISS index loaded")
else:
    vectorstore = FAISS.from_documents(chunks, embeddings)
    vectorstore.save_local(FAISS_FOLDER)
    print("‚úÖ FAISS index created and saved")

"""## **2. Hybrid Memory with FAISS Retriever**"""

import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# üîë Configure Gemini API
genai.configure(api_key="Add your API key here")
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-pro",
    temperature=0,
    google_api_key="AIzaSyCNf6Xf1ZB8zQaNKUbTT_z7gy4NfsV63r0"  # üëà force API key usage
)
# 1. Reload FAISS index (from Phase 1)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

!pip install langchain-experimental

"""##**Adding Python REPL tool**"""

from langchain.agents import Tool, initialize_agent
from langchain.agents.agent_types import AgentType
from langchain_experimental.tools.python.tool import PythonREPLTool

# 1. Define your REPL tool (executes Python code)
repl_tool = PythonREPLTool()

# 2. Add the retriever as a tool (so agent can also search docs)
retriever_tool = Tool(
    name="DocsSearch",
    func=retriever.get_relevant_documents,
    description="Useful for answering questions about Python. Input should be a fully formed question."
)

# 3. Collect all tools into a list
tools = [repl_tool, retriever_tool]

# 4. Reuse your memory (optional for consistency)
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# 5. Build the agent using Gemini and tools
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # allows reasoning + tool choice
    memory=memory,
    verbose=True
)

# ‚úÖ Agent is ready
print("ü§ñ Python Agent with REPL Tool is ready!")

agent.invoke('what is 3239*23456?')

